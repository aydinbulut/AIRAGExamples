# Intro
This example uses `multiply` and `get_waether`(dumy) tools to call if needed to generate the answer for the user prompt

# Requirements
This example needs
- `Ollama` for generating embeddings(vector) for semantic search, and also for generating LLM response. Used version `0.6.5`
    - Model `llama3.2` is needed to generate LLM response
- `NodeJS` of course :) Used version `20.15.0`

# Get started
- Start `Ollama` and load `llama3.2` model
- Run `npm i`
- Run `node llmWithTools.js`